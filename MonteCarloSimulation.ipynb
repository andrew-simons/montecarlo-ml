{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d283a0-6767-4cde-a1a6-d66d6436698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class InvalidTypeError(Exception):\n",
    "    \"\"\"enter either a call or put option for the type param\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d8a04",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- make a portfolio with weights\n",
    "- run monte carlo simulation\n",
    "\n",
    "\n",
    "Step 1: Get baseline monte carlo simulation with constant volatility (we use today's volatility for all steps in the MC simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9310a6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: treasury_data/3_month_treasury_constant_maturity_yield.csv\n",
      "Saved to: treasury_data/2_year_treasury_constant_maturity_yield.csv\n",
      "Saved to: treasury_data/10_year_treasury_constant_maturity_yield.csv\n",
      "r=0.03575316970581784\n",
      "AAPL\n",
      "Saved to: stock_data/AAPL_alphavantage_daily.csv\n",
      "S0 = 272.41\n",
      "sigma (annualized) = 0.2029029991092106\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'declaration_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'declaration_date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 137\u001b[0m\n\u001b[1;32m    135\u001b[0m df_div \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(raw_div)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeclaration_date\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mex_dividend_date\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayment_date\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayment_date\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecord_date\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 137\u001b[0m     df_div[col] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_div[col], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    138\u001b[0m df_div \u001b[38;5;241m=\u001b[39m df_div\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mex_dividend_date\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    139\u001b[0m df_div \u001b[38;5;241m=\u001b[39m df_div\u001b[38;5;241m.\u001b[39mset_index(df_div[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mex_dividend_date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'declaration_date'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from secrets import ALPHAVANTAGE_API_KEY\n",
    "\n",
    "\n",
    "# ALPHAVANTAGE_API_KEY = 'XJ2WQUNNHBYG6D9S'\n",
    "NUM_MINS = '1'\n",
    "\n",
    "################################################################################\n",
    "#                           DOWNLOAD FUNCTION                                    \n",
    "################################################################################\n",
    "def download_once(url: str, local_path: str, repull=False) -> Path:\n",
    "    p = Path(local_path)\n",
    "    if p.exists() and not repull: # do not rewrite path\n",
    "        return p\n",
    "    r = requests.get(url, timeout=30) # sends http get request to url. stores it to a response object\n",
    "    r.raise_for_status() # checks if response object's status is 200-299 (not 404 for example)\n",
    "    p.write_bytes(r.content) \n",
    "    return p\n",
    "\n",
    "################################################################################\n",
    "#                           CALCULATE PAYOFF FUNCTION                                   \n",
    "################################################################################\n",
    "# helper function\n",
    "def calculate_next_price(price_init, risk_free_rate, div_yield, volatility, maturity, time_steps):\n",
    "    dt = maturity/time_steps\n",
    "    normal_RV = np.random.normal()\n",
    "    price_next = price_init*np.exp((risk_free_rate - div_yield - 0.5*volatility**2)*dt + volatility*np.sqrt(dt)*normal_RV)\n",
    "    return price_next\n",
    "def calculate_payoff(price_init, risk_free_rate, div_yield, volatility, strike, maturity, time_steps, type=\"Call\"):\n",
    "    data = {'Day': [0,],\n",
    "            'Price': [price_init,],}\n",
    "    \n",
    "    next_price = None\n",
    "    for step in range(time_steps):\n",
    "        next_price = calculate_next_price(price_init, risk_free_rate, div_yield, volatility, maturity, time_steps)\n",
    "        price_init = next_price\n",
    "        data['Price'].append(next_price)\n",
    "        data['Day'].append(1+step)\n",
    "    df = pd.DataFrame(data)\n",
    "    if type == \"Call\":\n",
    "        return max(next_price - strike, 0), df\n",
    "    elif type == \"Put\":\n",
    "        return max(strike - next_price, 0), df\n",
    "    else:\n",
    "        raise InvalidTypeError\n",
    "def calculate_MC_estimator(price_init: float, risk_free_rate: float, div_yield: float, volatility: float, strike: float, maturity: float, num_simulations: int, time_steps: int, ticker: str, type=\"Call\") -> tuple[float, float, pd.DataFrame]: \n",
    "    payoff_list = []\n",
    "    list_of_df = []\n",
    "    for iteration_num in range(num_simulations):\n",
    "        payoff, df_i = calculate_payoff(price_init, risk_free_rate, div_yield, volatility, strike, maturity, time_steps, type)\n",
    "        payoff_list.append(payoff)\n",
    "        df_i['Iteration Number'] = iteration_num\n",
    "        list_of_df.append(df_i)\n",
    "    df_MC = pd.concat(list_of_df, ignore_index=True)\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(payoff_list, bins=100)\n",
    "    ax.set(title=f'Monte Carlo Simulation for {ticker} (European Option {type})', xlabel='Payoff (in dollars)', ylabel='Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    discount_factor = np.exp(-1*risk_free_rate*maturity)\n",
    "    MC_estimator = discount_factor*sum(payoff_list)/num_simulations\n",
    "    standard_error = discount_factor*np.std(payoff_list, ddof=1)/np.sqrt(num_simulations)\n",
    "    return MC_estimator, standard_error, df_MC\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                           Get risk-free rate                                     \n",
    "################################################################################\n",
    "_3_month_treasury_constant_maturity_yield_url = \"https://fred.stlouisfed.org/graph/fredgraph.csv?id=DGS3MO\"\n",
    "_2_year_treasury_constant_maturity_yield_url = \"https://fred.stlouisfed.org/graph/fredgraph.csv?id=DGS2\"\n",
    "_10_year_treasury_constant_maturity_yield_url = \"https://fred.stlouisfed.org/graph/fredgraph.csv?id=DGS10\"\n",
    "csv_path_3m = download_once(_3_month_treasury_constant_maturity_yield_url, \"treasury_data/3_month_treasury_constant_maturity_yield.csv\")\n",
    "print(\"Saved to:\", csv_path_3m)\n",
    "csv_path_2y = download_once(_2_year_treasury_constant_maturity_yield_url, \"treasury_data/2_year_treasury_constant_maturity_yield.csv\")\n",
    "print(\"Saved to:\", csv_path_2y)\n",
    "csv_path_10y = download_once(_10_year_treasury_constant_maturity_yield_url, \"treasury_data/10_year_treasury_constant_maturity_yield.csv\")\n",
    "print(\"Saved to:\", csv_path_10y)\n",
    "\n",
    "raw_3m = np.genfromtxt(csv_path_3m, delimiter=\",\", names=True, dtype=None, encoding=\"utf-8\", filling_values=None)\n",
    "df_3m = pd.DataFrame(raw_3m)\n",
    "df_3m.columns = ['date', 'yield']\n",
    "risk_free_rate = np.log(1 + df_3m['yield'].iloc[-1]/100) # compounded continuously\n",
    "\n",
    "print(f\"r={risk_free_rate}\")\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                           MAIN PORTFOLIO                                     \n",
    "################################################################################\n",
    "\n",
    "# maps: tickers (str) --> weights (float)\n",
    "main_portfolio = {'AAPL': 0.3,}\n",
    "\n",
    "\n",
    "for ticker in main_portfolio.keys():\n",
    "    ticker = 'AAPL'\n",
    "    print(ticker)\n",
    "    ################################################################################\n",
    "    #                           Get spot price today (S0)                                     \n",
    "    ################################################################################\n",
    "    url_daily = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&interval={NUM_MINS}min&apikey={ALPHAVANTAGE_API_KEY}&datatype=csv'\n",
    "    csv_path_daily = download_once(url_daily, f\"stock_data/{ticker}_alphavantage_daily.csv\")\n",
    "    print(\"Saved to:\", csv_path_daily)\n",
    "    raw_daily = np.genfromtxt(csv_path_daily, delimiter=\",\", names=True, dtype=None, encoding=\"utf-8\")\n",
    "\n",
    "    # columns: timestamp,open,high,low,close,volume\n",
    "    df_daily = pd.DataFrame(raw_daily) \n",
    "    df_daily['timestamp'] = pd.to_datetime(df_daily['timestamp'])\n",
    "    DAYS_BEFORE_TODAY = 30\n",
    "    price_init = df_daily['close'].iloc[DAYS_BEFORE_TODAY]\n",
    "    simulation_start_date = df_daily['timestamp'].iloc[DAYS_BEFORE_TODAY]\n",
    "    print(f\"S0 = {price_init}\")\n",
    "\n",
    "    ################################################################################\n",
    "    #                           Get volatility (sigma [SD])                                    \n",
    "    ################################################################################\n",
    "    num_days = 126\n",
    "    prices_list = list(df_daily['close'].iloc[DAYS_BEFORE_TODAY-num_days:])\n",
    "    logret = np.diff(np.log(prices_list))\n",
    "    volatility = float(logret.std(ddof=1) * np.sqrt(252)) #252 trading days is standard for delta t\n",
    "    print(\"sigma (annualized) =\", volatility)\n",
    "    \n",
    "    ################################################################################\n",
    "    #                           Get dividend yield of the stock (d)                                    \n",
    "    ################################################################################\n",
    "    url_div = f'https://www.alphavantage.co/query?function=DIVIDENDS&symbol={ticker}&apikey={ALPHAVANTAGE_API_KEY}&datatype=csv'\n",
    "    csv_path_div = download_once(url_div, f\"stock_data/{ticker}_alphavantage_dividends.csv\")\n",
    "    raw_div = np.genfromtxt(csv_path_div, delimiter=\",\", names=True, dtype=None, encoding=\"utf-8\")\n",
    "    df_div = pd.DataFrame(raw_div)\n",
    "    for col in ['declaration_date','ex_dividend_date','payment_date','payment_date','record_date']:\n",
    "        df_div[col] = pd.to_datetime(df_div[col], errors='coerce')\n",
    "    df_div = df_div.sort_values(by='ex_dividend_date', ascending=True)\n",
    "    df_div = df_div.set_index(df_div['ex_dividend_date'])\n",
    "    dividends_start_date = df_div.index.asof(simulation_start_date)\n",
    "    div_amount = df_div['amount'].asof(simulation_start_date)\n",
    "    div_yield = np.log(div_amount*0.01 + 1)  # type: ignore  (# make continuous)\n",
    "    print(f\"dividend yield = {div_yield}\")\n",
    "\n",
    "\n",
    "    ################################################################################\n",
    "    #                           Calculate Monte Carlo Estimator                                    \n",
    "    ################################################################################\n",
    "    sns.set_theme()\n",
    "    strike, maturity, num_simulations, time_steps = price_init, 30/252, 100, 30\n",
    "    mean, SE, df_MC = calculate_MC_estimator(price_init, risk_free_rate, div_yield, volatility, strike, maturity, num_simulations, time_steps, ticker, type=\"Call\")\n",
    "    print(f\"MC_estimator={mean}, SE={SE}\")\n",
    "\n",
    "    mean, SE, df_MC = calculate_MC_estimator(price_init, risk_free_rate, div_yield, volatility, strike, maturity, num_simulations, time_steps, ticker, type=\"Put\")\n",
    "    print(f\"MC_estimator={mean}, SE={SE}\")\n",
    "\n",
    "    MC_untrained_grid = sns.relplot(data=df_MC, kind=\"line\", x='Day', y='Price', hue='Iteration Number')\n",
    "    MC_untrained_grid.figure.suptitle(f\"Monte Carlo Simulation of {ticker}\")\n",
    "    print(np.mean(df_MC['Price']))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e2b01",
   "metadata": {},
   "source": [
    "Now we train a ML model to predict the volatility based on some parameters. But first, let's redefine our pandas dataframe into tensor for pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a7acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cc7e6b",
   "metadata": {},
   "source": [
    "Step 2: MLP regression (maps Xt -> yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4830ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "# input: L=60 past returns, 3 rolling vols, log price -> volatility \n",
    "# (input dimention=64 -> output dimension=1)\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# define tensors X and y\n",
    "\n",
    "class VolatilityMLP_NN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)   # outputs vol\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "pred_log_sigma = model(X)\n",
    "pred_sigma = torch.exp(pred_log_sigma)\n",
    "\n",
    "y = torch.log(realized_vol)\n",
    "\n",
    "X = torch.tensor(X_np, dtype=torch.float32)\n",
    "y = torch.tensor(y_np, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_train, X_val, X_test = ...\n",
    "y_train, y_val, y_test = ...\n",
    "\n",
    "\n",
    "# training\n",
    "model = VolatilityMLP_NN(input_dim=X.shape[1])\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pred = model(X_train)\n",
    "    loss = criterion(pred, y_train)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = criterion(model(X_val), y_val)\n",
    "        print(f\"Epoch {epoch}: train={loss.item():.4f}, val={val_loss.item():.4f}\")\n",
    "\n",
    "# evaluate on test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_pred = model(X_test)\n",
    "    test_loss = criterion(test_pred, y_test)\n",
    "\n",
    "print(\"Test MSE:\", test_loss.item())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284bb467",
   "metadata": {},
   "source": [
    "Step 3: LSTM (takes in a sequence. learns from patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd52ef",
   "metadata": {},
   "source": [
    "Step 4: Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3811b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 ex_dividend_date declaration_date record_date payment_date  \\\n",
      "ex_dividend_date                                                              \n",
      "2012-08-09             2012-08-09              NaT         NaT          NaT   \n",
      "2012-11-07             2012-11-07              NaT         NaT          NaT   \n",
      "2013-02-07             2013-02-07              NaT         NaT          NaT   \n",
      "2013-05-09             2013-05-09              NaT         NaT          NaT   \n",
      "2013-08-08             2013-08-08              NaT         NaT          NaT   \n",
      "\n",
      "                  amount  \n",
      "ex_dividend_date          \n",
      "2012-08-09          2.65  \n",
      "2012-11-07          2.65  \n",
      "2013-02-07          2.65  \n",
      "2013-05-09          3.05  \n",
      "2013-08-08          3.05  \n",
      "2025-11-12 00:00:00\n",
      "2025-11-10 00:00:00\n",
      "0.26\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# data = {\n",
    "#     'Name': ['alice', 'bob', 'charlie'],\n",
    "#     'Age': [25, 30, 35],\n",
    "#     'City': ['New York', 'Los Angeles', 'Chicago']\n",
    "# }\n",
    "\n",
    "# # Create a DataFrame with default integer labels (0, 1, 2)\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# print(df)\n",
    "\n",
    "# # Access the row with the default label 1\n",
    "# # Note: 1 is treated as the *label*, not the integer position\n",
    "# print(\"Accessing row with label 1 (Bob):\")\n",
    "# print(df.loc[1])\n",
    "\n",
    "# # Set a different column as the index to create custom string labels\n",
    "# df = df.set_index('Name')\n",
    "\n",
    "# df.loc['bob', 'Age']\n",
    "\n",
    "\n",
    "url_div = f'https://www.alphavantage.co/query?function=DIVIDENDS&symbol={ticker}&apikey={ALPHAVANTAGE_API_KEY}&datatype=csv'\n",
    "csv_path_div = download_once(url_div, f\"stock_data/{ticker}_alphavantage_dividends.csv\")\n",
    "raw_div = np.genfromtxt(csv_path_div, delimiter=\",\", names=True, dtype=None, encoding=\"utf-8\")\n",
    "df_div = pd.DataFrame(raw_div)\n",
    "for col in ['declaration_date','ex_dividend_date','payment_date','payment_date','record_date']:\n",
    "    df_div[col] = pd.to_datetime(df_div[col], errors='coerce')\n",
    "\n",
    "\n",
    "df_div = df_div.sort_values(by='ex_dividend_date', ascending=True)\n",
    "df_div = df_div.set_index(df_div['ex_dividend_date'])\n",
    "dividends_start_date = df_div.index.asof(simulation_start_date)\n",
    "amount = df_div['amount'].asof(simulation_start_date)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
